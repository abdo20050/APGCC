{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APGCC Inference and Visualization Notebook\n",
    "\n",
    "This notebook is adapted from `predict__v2.py`. It allows for running inference on images or videos, visualizing the detected points, and projecting them onto a 2D map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.ndimage\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as standard_transforms\n",
    "from glob import glob\n",
    "from typing import List\n",
    "\n",
    "# --- Project-specific imports ---\n",
    "# Add the project root to the Python path to allow for absolute imports\n",
    "if os.path.abspath(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "from apgcc.tracker import Track, PointTracker\n",
    "from config import cfg, merge_from_file\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set the paths to your configuration file, model weights, and input data here. You can also adjust the detection threshold and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # Path to the model config file\n",
    "    config_file = './configs/SHHA_test.yml'\n",
    "    # Path to the trained model weights\n",
    "    weights = './output/SHHA_best.pth'\n",
    "    # Path to a single image, a directory of images, or a video file\n",
    "    input = \"C:/Users/abdulna/OneDrive - KAUST/codes/APGCC/apgcc/test_vid/different_lighting_output/input_1.mp4\"\n",
    "    # Directory to save visualized results. If None, results are shown on screen.\n",
    "    output_dir = './pred_output/video_results'\n",
    "    # Confidence threshold for point detection\n",
    "    threshold = 0.5\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3D Projection Setup (Camera Calibration)\n",
    "\n",
    "This section sets up a more accurate projection using camera intrinsics and extrinsics. It back-projects each detected pixel onto a 3D plane at a specified height (e.g., average person height) to get its real-world (X, Y) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get frame dimensions for camera center (cx, cy) ---\n",
    "VIDEO_PATH = args.input\n",
    "cap_calib = cv2.VideoCapture(VIDEO_PATH)\n",
    "ret_calib, frame_calib = cap_calib.read()\n",
    "cap_calib.release()\n",
    "\n",
    "if not ret_calib:\n",
    "    raise ValueError(\"Failed to read video frame for camera setup\")\n",
    "\n",
    "h, w = frame_calib.shape[:2]\n",
    "\n",
    "# --- Step 1: Camera intrinsics (example values, replace with your calibration results) ---\n",
    "K = np.array([\n",
    "    [(50*w)/36, 0, w / 2],   # fx, 0, cx\n",
    "    [0, (50*h)/24, h / 2],   # 0, fy, cy\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float32)\n",
    "K_inv = np.linalg.inv(K)\n",
    "\n",
    "# --- Step 2: Camera extrinsics (rotation + translation, from your calibration) ---\n",
    "# Example: camera is 5m high, tilted down, looking 10m ahead on the ground plane\n",
    "R_vec = np.array([0.3, 0, 0]) # Rotation vector (e.g., from solvePnP)\n",
    "R = cv2.Rodrigues(R_vec)[0]   # Convert to rotation matrix\n",
    "t = np.array([[0], [5], [10]], dtype=np.float32)  # Translation vector in world coords\n",
    "R_T = R.T\n",
    "cam_center = -R_T @ t\n",
    "\n",
    "# --- Step 3: Function to project pixel to a given plane (Z = const) ---\n",
    "def backproject_to_plane(u, v, plane_height, K_inv, R_T, cam_center):\n",
    "    # Pixel in normalized camera coordinates\n",
    "    pixel_h = np.array([u, v, 1.0])\n",
    "    ray_cam = K_inv @ pixel_h\n",
    "\n",
    "    # Transform ray to world coordinates\n",
    "    ray_world = R_T @ ray_cam\n",
    "\n",
    "    # Solve intersection with plane Z = plane_height\n",
    "    s = (plane_height - cam_center[2]) / ray_world[2]\n",
    "    world_point = cam_center + s * ray_world\n",
    "    return world_point.flatten()[:2] # Return only (X, Y)\n",
    "\n",
    "def project_to_map(points, plane_height=1.7):\n",
    "    \"\"\"Projects a list of image points to the world plane.\"\"\"\n",
    "    if len(points) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    world_points = []\n",
    "    for p in points:\n",
    "        u, v = p[:2] # Handle points that might have tracking IDs\n",
    "        wp = backproject_to_plane(u, v, plane_height, K_inv, R_T, cam_center)\n",
    "        wp = np.array([wp[0]+1, wp[1]])* 4 # Scale factor for better visualization\n",
    "        world_points.append(wp)\n",
    "        \n",
    "    return np.array(world_points)\n",
    "\n",
    "def draw_on_map(map_points, size=(w, h), scale=20):\n",
    "    map_img = np.ones((size[1], size[0], 3), dtype=np.uint8) * 255\n",
    "    for x, y in map_points:\n",
    "        px, py = int((x+10) * scale), int(y * scale)\n",
    "        if 0 <= px < size[0] and 0 <= py < size[1]:\n",
    "            cv2.circle(map_img, (px, py), 5, (0, 0, 255), -1)  # Draw circles in red\n",
    "\n",
    "    # Use a named window to allow resizing\n",
    "    cv2.imshow(\"Projected 2D Map\", map_img)\n",
    "    cv2.waitKey(1)\n",
    "    return map_img\n",
    "\n",
    "def draw_heatmap(map_points, size=(w, h), scale=20, writer=None, sigma=30):\n",
    "    heatmap = np.zeros((size[1], size[0]), dtype=np.float32)\n",
    "    for x, y in map_points:\n",
    "        px, py = int((x+10) * scale), int(y * scale)\n",
    "        if 0 <= px < size[0] and 0 <= py < size[1]:\n",
    "            heatmap[size[1] - py, px] += 1.0\n",
    "\n",
    "    # Smooth the heatmap\n",
    "    if heatmap.max() > 0:\n",
    "        heatmap = scipy.ndimage.gaussian_filter(heatmap, sigma=sigma)\n",
    "        heatmap = np.clip(heatmap / heatmap.max(), 0, 1)\n",
    "    \n",
    "    heatmap_color = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.write(heatmap_color)\n",
    "\n",
    "    # Use a named window to allow resizing\n",
    "    cv2.imshow(\"Projected Heatmap\", heatmap_color)\n",
    "    cv2.waitKey(1)\n",
    "    return heatmap_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "These are utility functions for preprocessing, inference, visualization, and point merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_by_axis_pad(the_list: List[List[int]]) -> List[int]:\n",
    "    \"\"\"Helper function to find max dimensions for padding.\"\"\"\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    # Pad to a multiple of 128, mimicking training behavior\n",
    "    block = 128\n",
    "    for i in range(2):\n",
    "        maxes[i+1] = ((maxes[i+1] - 1) // block + 1) * block\n",
    "    return maxes\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[torch.Tensor]):\n",
    "    \"\"\"\n",
    "    Pads a list of tensors to the same size to create a single batch tensor.\n",
    "    \"\"\"\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        max_size = max_by_axis_pad([list(img.shape) for img in tensor_list])\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        for img, pad_img in zip(tensor_list, tensor):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return tensor\n",
    "\n",
    "def preprocess_image(image_input, cfg):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses a single image for model inference.\n",
    "    - image_input: can be a file path (str) or a PIL Image object.\n",
    "    - Converts to RGB\n",
    "    - Scales if the image is too large (mimicking validation logic)\n",
    "    - Converts to a Tensor and normalizes\n",
    "    \"\"\"\n",
    "    if isinstance(image_input, str):\n",
    "        img = Image.open(image_input).convert('RGB')\n",
    "    else:\n",
    "        img = image_input.convert('RGB')\n",
    "    \n",
    "    # --- Mimic validation scaling from dataset.py ---\n",
    "    temp_tensor = standard_transforms.ToTensor()(img)\n",
    "    max_size = max(temp_tensor.shape[1:])\n",
    "    scale = 1.0\n",
    "    upper_bound = cfg.DATALOADER.UPPER_BOUNDER\n",
    "\n",
    "    if upper_bound != -1 and max_size > upper_bound:\n",
    "        scale = upper_bound / max_size\n",
    "    elif max_size > 2560:  # A reasonable default from the original codebase\n",
    "        scale = 2560 / max_size\n",
    "\n",
    "    # --- Define transforms ---\n",
    "    transform_list = []\n",
    "    if scale != 1.0:\n",
    "        new_w = int(img.width * scale)\n",
    "        new_h = int(img.height * scale)\n",
    "        transform_list.append(standard_transforms.Resize((new_h, new_w)))\n",
    "\n",
    "    transform_list.extend([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    transform = standard_transforms.Compose(transform_list)\n",
    "    \n",
    "    display_img = img\n",
    "    if scale != 1.0:\n",
    "        display_img = display_img.resize((new_w, new_h))\n",
    "\n",
    "    img_tensor = transform(img)\n",
    "    return display_img, img_tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(model, image_tensor, threshold):\n",
    "    \"\"\"\n",
    "    Runs the model on a preprocessed image tensor and returns the predicted points.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    samples = nested_tensor_from_tensor_list([image_tensor.to(device)])\n",
    "    outputs = model(samples)\n",
    "    \n",
    "    outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]\n",
    "    outputs_points = outputs['pred_points'][0]\n",
    "    \n",
    "    points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()\n",
    "    predict_cnt = len(points)\n",
    "    \n",
    "    return points, predict_cnt\n",
    "\n",
    "def visualize_results(image_to_display, points, count, image_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Visualizes the results for a single image using matplotlib.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_to_display)\n",
    "    plt.title(f'Predicted Count: {count}')\n",
    "    \n",
    "    if len(points) > 0:\n",
    "        # Handle tracked points which may have an ID\n",
    "        if isinstance(points[0], (list, tuple)) and len(points[0]) > 2:\n",
    "             plot_points = np.array([p[0] for p in points])\n",
    "        else:\n",
    "             plot_points = np.array(points)\n",
    "        plt.scatter(plot_points[:, 0], plot_points[:, 1], c='red', s=15, marker='o', alpha=0.8, edgecolors='none')\n",
    "            \n",
    "    plt.axis('off')\n",
    "    \n",
    "    if output_dir:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        filename = os.path.basename(image_path)\n",
    "        output_path = os.path.join(output_dir, f\"pred_{filename}\")\n",
    "        plt.savefig(output_path, bbox_inches='tight', pad_inches=0.1)\n",
    "        print(f\"Result saved to {output_path}\")\n",
    "        plt.close() # Close the figure to free memory\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def merge_close_points(points, threshold):\n",
    "    points = np.array(points)\n",
    "    if len(points) == 0:\n",
    "        return []\n",
    "\n",
    "    merged = []\n",
    "    visited = set()\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        if i in visited:\n",
    "            continue\n",
    "        cluster = [points[i]]\n",
    "        visited.add(i)\n",
    "\n",
    "        for j in range(i + 1, len(points)):\n",
    "            if j in visited:\n",
    "                continue\n",
    "            dist = np.linalg.norm(points[i] - points[j])\n",
    "            if dist < threshold:\n",
    "                cluster.append(points[j])\n",
    "                visited.add(j)\n",
    "\n",
    "        # Use the first point in the cluster as the representative\n",
    "        merged.append(cluster[0])\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading\n",
    "\n",
    "This cell sets up the device (GPU or CPU), builds the model architecture based on the config, and loads the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "### VGG16: last_pool= False\n",
      "ultra_pe\n",
      "auxiliary anchors: (pos, neg, lambda, kwargs)  [2, 2] [2, 8] {'pos_coef': 1.0, 'neg_coef': 1.0, 'pos_loc': 0.0002, 'neg_loc': 0.0002}\n",
      "Model weights loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdulna\\AppData\\Local\\Temp\\ipykernel_13564\\3619104338.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(args.weights, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "# --- Load Configuration ---\n",
    "if args.config_file != \"\":\n",
    "    cfg_ = merge_from_file(cfg, args.config_file)\n",
    "else:\n",
    "    cfg_ = cfg\n",
    "\n",
    "# --- Setup Model ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = build_model(cfg=cfg_, training=False)\n",
    "model.to(device)\n",
    "\n",
    "# --- Load Weights ---\n",
    "if not os.path.exists(args.weights):\n",
    "    raise FileNotFoundError(f\"Weight file not found: {args.weights}\")\n",
    "\n",
    "state_dict = torch.load(args.weights, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"Model weights loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Processing\n",
    "\n",
    "This is the main execution block. It checks if the input is a video or image(s) and processes them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: input_1.mp4\n",
      "Processing frame 1\n",
      "Processing frame 2\n",
      "Processing frame 3\n",
      "Processing frame 4\n",
      "Processing frame 5\n",
      "Processing frame 6\n",
      "Processing frame 7\n",
      "Processing frame 8\n",
      "Processing frame 9\n",
      "Processing frame 10\n",
      "Processing frame 11\n",
      "Processing frame 12\n",
      "Processing frame 13\n",
      "Processing frame 14\n",
      "Processing frame 15\n",
      "Processing frame 16\n",
      "Processing frame 17\n",
      "Processing frame 18\n",
      "Processing frame 19\n",
      "Processing frame 20\n",
      "Processing frame 21\n",
      "Processing frame 22\n",
      "Processing frame 23\n",
      "Processing frame 24\n",
      "Processing frame 25\n",
      "Processing frame 26\n",
      "Processing frame 27\n",
      "Processing frame 28\n",
      "Processing frame 29\n",
      "Processing frame 30\n",
      "Processing frame 31\n",
      "Processing frame 32\n",
      "Processing frame 33\n",
      "Processing frame 34\n",
      "Processing frame 35\n",
      "Processing frame 36\n",
      "Processing frame 37\n",
      "Processing frame 38\n",
      "Processing frame 39\n",
      "Processing frame 40\n",
      "Processing frame 41\n",
      "Processing frame 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\u001b[32m     42\u001b[39m _, img_tensor = preprocess_image(pil_img, cfg_)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m points, count = \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Collect points over a window of frames and then merge to stabilize\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(displayed_points) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mrun_inference\u001b[39m\u001b[34m(model, image_tensor, threshold)\u001b[39m\n\u001b[32m     80\u001b[39m device = \u001b[38;5;28mnext\u001b[39m(model.parameters()).device\n\u001b[32m     82\u001b[39m samples = nested_tensor_from_tensor_list([image_tensor.to(device)])\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m outputs_scores = torch.nn.functional.softmax(outputs[\u001b[33m'\u001b[39m\u001b[33mpred_logits\u001b[39m\u001b[33m'\u001b[39m], -\u001b[32m1\u001b[39m)[:, :, \u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     86\u001b[39m outputs_points = outputs[\u001b[33m'\u001b[39m\u001b[33mpred_points\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\OneDrive - KAUST\\codes\\APGCC\\apgcc\\models\\APGCC.py:74\u001b[39m, in \u001b[36mModel_builder.forward\u001b[39m\u001b[34m(self, samples)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples: NestedTensor):\n\u001b[32m     73\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.encoder(samples)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\OneDrive - KAUST\\codes\\APGCC\\apgcc\\models\\Decoder.py:225\u001b[39m, in \u001b[36mIFI_Decoder_Model.forward\u001b[39m\u001b[34m(self, samples, features)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# Transform output\u001b[39;00m\n\u001b[32m    224\u001b[39m offset *= \u001b[32m100\u001b[39m   \n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m anchor_points = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m.repeat(batch_size, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# get sample point map\u001b[39;00m\n\u001b[32m    226\u001b[39m output_coord = offset + anchor_points   \u001b[38;5;66;03m# [b, h/d*w/d*line*row, 2(x,y)]  # transfer feature coordinate moving to image coordinate. (correspond to head center)\u001b[39;00m\n\u001b[32m    227\u001b[39m output_confid = confidence              \u001b[38;5;66;03m# [b, h/d*w/d*line*row, 2(confidence)]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\AppData\\Local\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdulna\\OneDrive - KAUST\\codes\\APGCC\\apgcc\\models\\Decoder.py:34\u001b[39m, in \u001b[36mAnchorPoints.forward\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# send reference points to device\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_anchor_points\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.from_numpy(all_anchor_points.astype(np.float32))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Find Input Files ---\n",
    "input_path = args.input\n",
    "is_video = os.path.isfile(input_path) and any(input_path.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov'])\n",
    "\n",
    "if is_video:\n",
    "    # --- Process Video File ---\n",
    "    video_path = input_path\n",
    "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "    out = None\n",
    "    if args.output_dir:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        output_video_path = os.path.join(args.output_dir, os.path.splitext(os.path.basename(video_path))[0] + \"_output.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Use a widely supported codec\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width * 2, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    tracker = PointTracker(distance_threshold=100) # Simple distance-based tracker\n",
    "    collected_points = []\n",
    "    displayed_points = np.empty((0, 2))\n",
    "    window = 2  # Number of frames to collect points before merging\n",
    "    \n",
    "    # Create named windows that can be resized\n",
    "    cv2.namedWindow(\"Combined Output\", cv2.WINDOW_NORMAL)\n",
    "    cv2.namedWindow(\"Projected Heatmap\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        print(f\"Processing frame {frame_count}\")\n",
    "\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        _, img_tensor = preprocess_image(pil_img, cfg_)\n",
    "        points, count = run_inference(model, img_tensor, args.threshold)\n",
    "\n",
    "        # Collect points over a window of frames and then merge to stabilize\n",
    "        if len(displayed_points) == 0:\n",
    "            displayed_points = np.array(points)\n",
    "        if frame_count % window:\n",
    "            collected_points.extend(points.tolist())\n",
    "        else:\n",
    "            displayed_points = merge_close_points(collected_points, threshold=5)\n",
    "            collected_points = []\n",
    "\n",
    "        # Track points (optional, using simple tracker for now)\n",
    "        # tracked_points = tracker.update(points)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        map_coords = project_to_map(displayed_points, plane_height=1.7) # Assume avg person height is 1.7m\n",
    "        points_map = draw_on_map(map_coords, size=(w,h))\n",
    "        heatmap = draw_heatmap(map_coords, size=(w,h))\n",
    "\n",
    "        # Draw points on the original frame\n",
    "        for point in displayed_points:\n",
    "            x, y = int(point[0]), int(point[1])\n",
    "            cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)  # Red circles\n",
    "\n",
    "        cv2.putText(frame, f\"Count: {len(displayed_points)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Combine frame and heatmap for display and saving\n",
    "        # combined_frame = np.concatenate((frame, heatmap), axis=1)\n",
    "        combined_frame = np.concatenate((frame, points_map), axis=1)\n",
    "        \n",
    "        cv2.imshow('Combined Output', combined_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        if out:\n",
    "            out.write(combined_frame)\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "        print(f\"Video with predictions saved to {output_video_path}\")\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "else:\n",
    "    # --- Process Image(s) ---\n",
    "    if os.path.isdir(input_path):\n",
    "        image_files = []\n",
    "        supported_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "        for ext in supported_extensions:\n",
    "            image_files.extend(glob(os.path.join(input_path, ext)))\n",
    "        image_files = sorted(image_files)\n",
    "    elif os.path.isfile(input_path):\n",
    "        image_files = [input_path]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Input not found: {input_path}\")\n",
    "\n",
    "    print(f\"Found {len(image_files)} image(s) to process.\")\n",
    "\n",
    "    for image_path in image_files:\n",
    "        print(f\"\\nProcessing: {os.path.basename(image_path)}\")\n",
    "        display_img, img_tensor = preprocess_image(image_path, cfg_)\n",
    "        points, count = run_inference(model, img_tensor, args.threshold)\n",
    "        visualize_results(display_img, points, count, image_path, args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
