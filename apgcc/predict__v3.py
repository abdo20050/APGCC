# ========================
# APGCC Inference - With Evaluation Metrics (Tracked Points)
# ========================
import cv2
import numpy as np
import scipy.ndimage
import scipy.optimize
import scipy.spatial
import os
import csv
import json
os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'
import argparse
import torch
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as standard_transforms
from glob import glob
from typing import List, Tuple, Dict
from collections import defaultdict
import sys
import math

# --- Kalman Import ---
# Ensure kalman_tracker.py is in the same folder or python path
try:
    from kalman_tracker import SortPointTracker
except ImportError:
    print("Warning: kalman_tracker module not found. Tracking disabled.")
    SortPointTracker = None

# --- Project-specific imports ---
# Ensure these match your folder structure
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from config import cfg, merge_from_file
    from models import build_model
except ImportError:
    print("Error: Could not import project config/models. Run this script from the project root.")
    sys.exit(1)

# ==========================================
#   ACCURACY / EVALUATION UTILITIES
# ==========================================

def load_gt_data(csv_path: str) -> Dict[int, List[Tuple[int, int]]]:
    """
    Reads the CSV file generated by Blender.
    Returns: { frame_number: [(x, y), (x, y), ...], ... }
    """
    points_map = defaultdict(list)
    if not csv_path or not os.path.exists(csv_path):
        print(f"Warning: GT CSV not found at {csv_path}. Evaluation will be skipped.")
        return points_map
        
    print(f"Loading Ground Truth from {csv_path}...")
    try:
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    frame = int(row['Frame'])
                    x = int(row['X'])
                    y = int(row['Y'])
                    points_map[frame].append((x, y))
                except (ValueError, KeyError):
                    continue
    except Exception as e:
        print(f"Error reading CSV: {e}")
        
    return points_map

def compute_metrics(pred_points, gt_points, dist_threshold=20.0):
    """
    Calculates TP, FP, FN, and MAE for a single frame using Linear Sum Assignment.
    
    Args:
        pred_points: List of [x, y] or np.array shape (N, 2)
        gt_points: List of [x, y] or np.array shape (M, 2)
        dist_threshold: Max pixels allowed for a match to be valid.
        
    Returns:
        dict with counts and errors
    """
    # Convert to numpy arrays if not already
    preds = np.array(pred_points) if len(pred_points) > 0 else np.empty((0, 2))
    gts = np.array(gt_points) if len(gt_points) > 0 else np.empty((0, 2))
    
    tp = 0
    fp = 0
    fn = 0
    total_error = 0.0
    
    # Case 1: No Predictions, No GT
    if len(preds) == 0 and len(gts) == 0:
        return {'tp': 0, 'fp': 0, 'fn': 0, 'error': 0.0, 'matches': 0}
        
    # Case 2: No Predictions, but we have GT
    if len(preds) == 0:
        return {'tp': 0, 'fp': 0, 'fn': len(gts), 'error': 0.0, 'matches': 0}

    # Case 3: Predictions, but no GT
    if len(gts) == 0:
        return {'tp': 0, 'fp': len(preds), 'fn': 0, 'error': 0.0, 'matches': 0}
        
    # Case 4: Both exist - Perform Matching
    
    # Calculate distance matrix (N x M)
    d_matrix = scipy.spatial.distance.cdist(preds, gts, metric='euclidean')
    
    # Hungarian Algorithm (Linear Sum Assignment)
    # Finds the row_ind (preds) and col_ind (gts) that minimize total distance
    row_ind, col_ind = scipy.optimize.linear_sum_assignment(d_matrix)
    
    # Filter matches by threshold
    used_preds = set()
    used_gts = set()
    
    for r, c in zip(row_ind, col_ind):
        dist = d_matrix[r, c]
        if dist < dist_threshold:
            tp += 1
            total_error += dist
            used_preds.add(r)
            used_gts.add(c)
    
    # False Positives: Predictions that were not matched or were too far
    fp = len(preds) - len(used_preds)
    
    # False Negatives: GTs that were not matched
    fn = len(gts) - len(used_gts)
    
    return {
        'tp': tp,
        'fp': fp,
        'fn': fn,
        'error': total_error,
        'matches': tp # Used for MAE calculation
    }

# ==========================================
#   EXISTING MODEL UTILITIES
# ==========================================

def max_by_axis_pad(the_list: List[List[int]]) -> List[int]:
    maxes = the_list[0]
    for sublist in the_list[1:]:
        for index, item in enumerate(sublist):
            maxes[index] = max(maxes[index], item)
    block = 128
    for i in range(2):
        maxes[i+1] = ((maxes[i+1] - 1) // block + 1) * block
    return maxes

def nested_tensor_from_tensor_list(tensor_list: List[torch.Tensor]):
    if tensor_list[0].ndim == 3:
        max_size = max_by_axis_pad([list(img.shape) for img in tensor_list])
        batch_shape = [len(tensor_list)] + max_size
        b, c, h, w = batch_shape
        dtype = tensor_list[0].dtype
        device = tensor_list[0].device
        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)
        for img, pad_img in zip(tensor_list, tensor):
            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)
    else: raise ValueError('not supported')
    return tensor

def get_args():
    # Get the directory where this script is located
    script_dir = os.path.dirname(os.path.abspath(__file__))

    parser = argparse.ArgumentParser('APGCC Inference Script with Evaluation')
    parser.add_argument('-c', '--config_file', type=str, default=os.path.join(script_dir, "configs/SHHA_test.yml"))
    parser.add_argument('-w', '--weights', type=str, default=os.path.join(script_dir, "output/SHHA_best.pth"))
    parser.add_argument('-i', '--input', required=True, type=str)
    parser.add_argument('-o', '--output_dir', type=str, default=os.path.join(script_dir, "pred_output"))
    parser.add_argument('--threshold', type=float, default=0.5)
    # --- NEW ARGUMENT ---
    parser.add_argument('--gt_csv', type=str, default=None, help="Path to pixel_coords.csv. Defaults to <input_filename>.csv if available.")
    parser.add_argument('--match_dist', type=float, default=25.0, help="Distance threshold (pixels) for matching GT to Pred")
    return parser.parse_args()

def preprocess_image(image_input, cfg):
    if isinstance(image_input, str): img = Image.open(image_input).convert('RGB')
    else: img = image_input.convert('RGB')
    temp_tensor = standard_transforms.ToTensor()(img)
    max_size = max(temp_tensor.shape[1:])
    scale = 1.0
    upper_bound = cfg.DATALOADER.UPPER_BOUNDER
    if upper_bound != -1 and max_size > upper_bound: scale = upper_bound / max_size
    elif max_size > 2560: scale = 2560 / max_size
    transform_list = []
    new_w, new_h = img.width, img.height
    if scale != 1.0:
        new_w = int(img.width * scale)
        new_h = int(img.height * scale)
        transform_list.append(standard_transforms.Resize((new_h, new_w)))
    transform_list.extend([
        standard_transforms.ToTensor(),
        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    transform = standard_transforms.Compose(transform_list)
    display_img = img
    if scale != 1.0: display_img = display_img.resize((new_w, new_h))
    img_tensor = transform(img)
    return display_img, img_tensor

@torch.no_grad()
def run_inference(model, image_tensor, threshold):
    model.eval()
    device = next(model.parameters()).device
    samples = nested_tensor_from_tensor_list([image_tensor.to(device)])
    outputs = model(samples)
    outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]
    outputs_points = outputs['pred_points'][0]
    points = outputs_points[outputs_scores > threshold].detach().cpu().numpy()
    return points, len(points)

def main():
    args = get_args()
    if args.config_file != "": cfg_ = merge_from_file(cfg, args.config_file)
    else: cfg_ = cfg
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    model = build_model(cfg=cfg_, training=False)
    model.to(device)
    if not os.path.exists(args.weights): raise FileNotFoundError(f"Weight file not found: {args.weights}")
    model.load_state_dict(torch.load(args.weights, map_location='cpu'))
    print("Model weights loaded.")

    # --- Load Ground Truth if provided or auto-detected ---
    gt_data = {}
    
    # Auto-detect logic: if --gt_csv is not set, look for {video_name}.csv
    if args.gt_csv is None:
        if os.path.isfile(args.input):
            # e.g. /path/to/video.mp4 -> /path/to/video.csv
            candidate_csv = os.path.splitext(args.input)[0] + ".csv"
            if os.path.exists(candidate_csv):
                print(f"Auto-detected Ground Truth CSV: {candidate_csv}")
                args.gt_csv = candidate_csv
    
    if args.gt_csv:
        gt_data = load_gt_data(args.gt_csv)
    
    # --- Initialize Metrics Accumulators (Dual Mode) ---
    # Raw: Model output only
    stats_raw = {
        'tp': 0, 'fp': 0, 'fn': 0, 'error': 0.0, 'matches': 0,
        'frame_data': []
    }
    # Tracked: Model + Kalman output
    stats_trk = {
        'tp': 0, 'fp': 0, 'fn': 0, 'error': 0.0, 'matches': 0,
        'frame_data': []
    }

    if os.path.isdir(args.input): image_files = sorted(glob(os.path.join(args.input, '*.jpg')))
    elif os.path.isfile(args.input): image_files = [args.input]
    else: raise FileNotFoundError(f"Input not found: {args.input}")

    if len(image_files) == 1 and any(ext in image_files[0] for ext in ['.mp4', '.avi', '.mov']):
        video_path = image_files[0]
        print(f"Processing video: {os.path.basename(video_path)}")
        cap = cv2.VideoCapture(video_path)
        
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        if args.output_dir:
            if not os.path.exists(args.output_dir): os.makedirs(args.output_dir)
            output_video_path = os.path.join(args.output_dir, os.path.splitext(os.path.basename(video_path))[0] + "_tracked.mp4")
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # --- CODEC FALLBACK MECHANISM ---
            # 1. Try 'avc1' (H.264) - Best for browsers/VS Code, but requires openh264 DLL
            # 2. Try 'mp4v' (MPEG-4) - Most compatible with OpenCV default installation
            # 3. Try 'MJPG' (Motion JPEG) - Last resort fallback
            codecs_to_try = ['avc1', 'mp4v', 'MJPG']
            out = None
            
            for codec in codecs_to_try:
                try:
                    fourcc = cv2.VideoWriter_fourcc(*codec)
                    temp_out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))
                    
                    if temp_out.isOpened():
                        print(f"\n[INFO] Video Writer initialized successfully using codec: {codec}")
                        out = temp_out
                        break
                    else:
                        print(f"\n[WARNING] Failed to initialize codec '{codec}'. Trying next...")
                except Exception as e:
                     print(f"\n[WARNING] Error with codec '{codec}': {e}")
            
            if out is None:
                print("\n[ERROR] Could not initialize any video codec. Video saving will be DISABLED.")

        else: out = None

        frame_count = 0
        
        # --- TRACKER ---
        if SortPointTracker:
            tracker = SortPointTracker(max_age=40, min_hits=3, distance_threshold=80) 
        else:
            tracker = None
        
        if gt_data:
            print("\n" + "="*80)
            print("Real-time Stats: Displaying TRACKED results per frame.")
            print(f"{'Frame':<8} | {'TP':<6} {'FP':<6} {'FN':<6} | {'Precision':<10} {'Recall':<10} {'F1-Score':<10} | {'MAE (px)':<10}")
            print("="*80)
        else:
            print("\nRunning standard inference (No Ground Truth CSV found).")

        while True:
            ret, frame = cap.read()
            if not ret: break
            
            current_frame_num = frame_count + 1 
            frame_count += 1
            
            pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            display_img, img_tensor = preprocess_image(pil_img, cfg_)
            
            # --- 1. INFERENCE (Get Raw Points) ---
            raw_points, count = run_inference(model, img_tensor, args.threshold)
            
            # --- 2. TRACKING (Get Tracked Points) ---
            if tracker:
                # tracked_points format: [x, y, id, is_predicted]
                tracked_points = tracker.update(raw_points)
            else:
                tracked_points = [[p[0], p[1], i, 0] for i, p in enumerate(raw_points)]
            
            # Extract just (x, y) from tracked points for evaluation
            eval_points_tracked = [p[:2] for p in tracked_points]

            # --- 3. DUAL EVALUATION ---
            if gt_data:
                current_gt = gt_data.get(current_frame_num, [])
                
                # A) Evaluate RAW Points
                m_raw = compute_metrics(raw_points, current_gt, dist_threshold=args.match_dist)
                
                stats_raw['tp'] += m_raw['tp']
                stats_raw['fp'] += m_raw['fp']
                stats_raw['fn'] += m_raw['fn']
                stats_raw['error'] += m_raw['error']
                stats_raw['matches'] += m_raw['matches']
                
                # Calc Raw Frame Stats
                p_r = m_raw['tp'] / (m_raw['tp'] + m_raw['fp']) if (m_raw['tp'] + m_raw['fp']) > 0 else 0
                r_r = m_raw['tp'] / (m_raw['tp'] + m_raw['fn']) if (m_raw['tp'] + m_raw['fn']) > 0 else 0
                f1_r = 2 * (p_r * r_r) / (p_r + r_r) if (p_r + r_r) > 0 else 0
                mae_r = m_raw['error'] / m_raw['matches'] if m_raw['matches'] > 0 else 0
                stats_raw['frame_data'].append({'p': p_r, 'r': r_r, 'f1': f1_r, 'mae': mae_r})

                # B) Evaluate TRACKED Points
                m_trk = compute_metrics(eval_points_tracked, current_gt, dist_threshold=args.match_dist)
                
                stats_trk['tp'] += m_trk['tp']
                stats_trk['fp'] += m_trk['fp']
                stats_trk['fn'] += m_trk['fn']
                stats_trk['error'] += m_trk['error']
                stats_trk['matches'] += m_trk['matches']
                
                # Calc Tracked Frame Stats
                p_t = m_trk['tp'] / (m_trk['tp'] + m_trk['fp']) if (m_trk['tp'] + m_trk['fp']) > 0 else 0
                r_t = m_trk['tp'] / (m_trk['tp'] + m_trk['fn']) if (m_trk['tp'] + m_trk['fn']) > 0 else 0
                f1_t = 2 * (p_t * r_t) / (p_t + r_t) if (p_t + r_t) > 0 else 0
                mae_t = m_trk['error'] / m_trk['matches'] if m_trk['matches'] > 0 else 0
                stats_trk['frame_data'].append({'p': p_t, 'r': r_t, 'f1': f1_t, 'mae': mae_t})

                # Print Stats Per Frame (Using Tracked for display)
                print(f"{current_frame_num:<8} | {m_trk['tp']:<6} {m_trk['fp']:<6} {m_trk['fn']:<6} | {p_t:<10.2f} {r_t:<10.2f} {f1_t:<10.2f} | {mae_t:<10.2f}")
                
                # --- DRAW METRICS ON VIDEO (Tracked) ---
                info_text = [
                    f"TP: {m_trk['tp']}  FP: {m_trk['fp']}  FN: {m_trk['fn']}",
                    f"Prec: {p_t:.2f}  Rec: {r_t:.2f}  F1: {f1_t:.2f}",
                    f"MAE: {mae_t:.1f}px"
                ]
                
                text_start_y = frame_height - 90
                for i, line in enumerate(info_text):
                    y_pos = text_start_y + (i * 30)
                    cv2.putText(frame, line, (15, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 4)
                    cv2.putText(frame, line, (15, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                
                # --- DRAW GROUND TRUTH (White Squares) ---
                box_size = 12  # Half-width (Total width = 24px)
                for gx, gy in current_gt:
                    # Draw White Rectangle (Thickness 1)
                    cv2.rectangle(frame, (gx - box_size, gy - box_size), (gx + box_size, gy + box_size), (255, 255, 255), 1)

            if out:
                for point in tracked_points:
                    x, y, tid, is_pred = int(point[0]), int(point[1]), int(point[2]), int(point[3])
                    if not (0 <= x < frame_width and 0 <= y < frame_height): continue

                    if is_pred == 0: 
                        # Real: Red Circle, Green Text
                        cv2.circle(frame, (x, y), 4, (0, 0, 255), -1) 
                        cv2.putText(frame, f"ID:{tid}", (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                    else: 
                        # Ghost: Cyan Circle, Yellow Text
                        cv2.circle(frame, (x, y), 3, (255, 255, 0), -1) 
                        cv2.putText(frame, f"ID:{tid}", (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)

                # --- DRAW COUNTS (Real vs Pred) ---
                pred_count = len(tracked_points)
                if gt_data:
                    real_count = len(gt_data.get(current_frame_num, []))
                    count_text = f"Real Count: {real_count} | Pred Count: {pred_count}"
                else:
                    count_text = f"Pred Count: {pred_count}"

                # Draw text with outline for better visibility against any background
                cv2.putText(frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 4) # Black Outline
                cv2.putText(frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) # Green Text
                
                out.write(frame)

        cap.release()
        if out: out.release()
        print(f"\nVideo saved to {output_video_path}")
        
        # --- FINAL METRICS REPORT ---
        if gt_data and len(stats_raw['frame_data']) > 0:
            def calc_final(stats):
                # Micro
                tot_tp, tot_fp, tot_fn = stats['tp'], stats['fp'], stats['fn']
                mic_p = tot_tp / (tot_tp + tot_fp) if (tot_tp + tot_fp) > 0 else 0
                mic_r = tot_tp / (tot_tp + tot_fn) if (tot_tp + tot_fn) > 0 else 0
                mic_f1 = 2 * (mic_p * mic_r) / (mic_p + mic_r) if (mic_p + mic_r) > 0 else 0
                mic_mae = stats['error'] / stats['matches'] if stats['matches'] > 0 else 0
                
                # Macro
                f_data = stats['frame_data']
                mac_p = sum(x['p'] for x in f_data) / len(f_data)
                mac_r = sum(x['r'] for x in f_data) / len(f_data)
                mac_f1 = sum(x['f1'] for x in f_data) / len(f_data)
                mac_mae = sum(x['mae'] for x in f_data) / len(f_data)
                
                return (mic_p, mic_r, mic_f1, mic_mae), (mac_p, mac_r, mac_f1, mac_mae)

            raw_micro, raw_macro = calc_final(stats_raw)
            trk_micro, trk_macro = calc_final(stats_trk)
            
            print("\n" + "="*80)
            print("   COMPARATIVE EVALUATION REPORT (Raw Model vs. Tracked)")
            print("="*80)
            print(f"{'METRIC':<20} | {'RAW (No Tracker)':<20} | {'TRACKED (Kalman)':<20} | {'DIFF':<10}")
            print("-" * 80)
            
            def print_comp(name, val_raw, val_trk, is_percent=True, is_err=False):
                 diff = val_trk - val_raw
                 fmt = ".4f" if is_percent else ".2f"
                 suffix = " px" if not is_percent else ""
                 
                 s_raw = f"{val_raw:{fmt}}{suffix}"
                 s_trk = f"{val_trk:{fmt}}{suffix}"
                 
                 if abs(diff) < 0.0001: s_diff = "="
                 elif is_err: 
                     s_diff = f"{diff:{fmt}}" 
                 else: 
                     s_diff = f"{'+' if diff > 0 else ''}{diff:{fmt}}"
                     
                 print(f"{name:<20} | {s_raw:<20} | {s_trk:<20} | {s_diff:<10}")

            print("--- MICRO-AVERAGE (Overall Totals) ---")
            print_comp("Precision", raw_micro[0], trk_micro[0])
            print_comp("Recall", raw_micro[1], trk_micro[1])
            print_comp("F1 Score", raw_micro[2], trk_micro[2])
            print_comp("MAE (Error)", raw_micro[3], trk_micro[3], is_percent=False, is_err=True)
            
            print("\n--- MACRO-AVERAGE (Per-Frame Average) ---")
            print_comp("Precision", raw_macro[0], trk_macro[0])
            print_comp("Recall", raw_macro[1], trk_macro[1])
            print_comp("F1 Score", raw_macro[2], trk_macro[2])
            print_comp("MAE (Error)", raw_macro[3], trk_macro[3], is_percent=False, is_err=True)
            print("="*80)
            
            def round3(val):
                return round(val, 3)

            # --- EXPORT TO JSON ---
            json_report = {
                "video_file": os.path.basename(video_path),
                "total_frames_analyzed": len(stats_raw['frame_data']),
                "comparison": {
                    "raw_model": {
                        "micro_average": {
                            "precision": round3(raw_micro[0]),
                            "recall": round3(raw_micro[1]),
                            "f1_score": round3(raw_micro[2]),
                            "mae_pixels": round3(raw_micro[3])
                        },
                        "macro_average": {
                            "precision": round3(raw_macro[0]),
                            "recall": round3(raw_macro[1]),
                            "f1_score": round3(raw_macro[2]),
                            "mae_pixels": round3(raw_macro[3])
                        }
                    },
                    "tracked_model": {
                        "micro_average": {
                            "precision": round3(trk_micro[0]),
                            "recall": round3(trk_micro[1]),
                            "f1_score": round3(trk_micro[2]),
                            "mae_pixels": round3(trk_micro[3])
                        },
                        "macro_average": {
                            "precision": round3(trk_macro[0]),
                            "recall": round3(trk_macro[1]),
                            "f1_score": round3(trk_macro[2]),
                            "mae_pixels": round3(trk_macro[3])
                        }
                    }
                }
            }

            # Determine Output Paths
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            if args.output_dir:
                json_path = os.path.join(args.output_dir, f"{base_name}_metrics.json")
                csv_path = os.path.join(args.output_dir, f"{base_name}_comparison.csv")
            else:
                json_path = f"{base_name}_metrics.json"
                csv_path = f"{base_name}_comparison.csv"

            # 1. Save JSON
            try:
                with open(json_path, 'w') as f:
                    json.dump(json_report, f, indent=4)
                print(f"\nReport exported to: {json_path}")
            except Exception as e:
                print(f"\nError exporting JSON: {e}")

            # 2. Save Comparison CSV
            try:
                with open(csv_path, 'w', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(["Metric", "Aggregation", "Raw Model", "Tracked Model", "Difference"])
                    
                    # Micro Metrics
                    writer.writerow(["Precision", "Micro", round3(raw_micro[0]), round3(trk_micro[0]), round3(trk_micro[0] - raw_micro[0])])
                    writer.writerow(["Recall", "Micro", round3(raw_micro[1]), round3(trk_micro[1]), round3(trk_micro[1] - raw_micro[1])])
                    writer.writerow(["F1 Score", "Micro", round3(raw_micro[2]), round3(trk_micro[2]), round3(trk_micro[2] - raw_micro[2])])
                    writer.writerow(["MAE", "Micro", round3(raw_micro[3]), round3(trk_micro[3]), round3(trk_micro[3] - raw_micro[3])])
                    
                    # Macro Metrics
                    writer.writerow(["Precision", "Macro", round3(raw_macro[0]), round3(trk_macro[0]), round3(trk_macro[0] - raw_macro[0])])
                    writer.writerow(["Recall", "Macro", round3(raw_macro[1]), round3(trk_macro[1]), round3(trk_macro[1] - raw_macro[1])])
                    writer.writerow(["F1 Score", "Macro", round3(raw_macro[2]), round3(trk_macro[2]), round3(trk_macro[2] - raw_macro[2])])
                    writer.writerow(["MAE", "Macro", round3(raw_macro[3]), round3(trk_macro[3]), round3(trk_macro[3] - raw_macro[3])])
                    
                print(f"Comparison CSV exported to: {csv_path}")
            except Exception as e:
                print(f"\nError exporting CSV: {e}")

if __name__ == '__main__':
    main()